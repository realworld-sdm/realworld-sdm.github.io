
<!DOCTYPE html>
<html lang="en">

<head>
    <link rel="shortcut icon" href="favicon.ico?">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Real-world Sequential Decision Making workshop, co-located with ICML 2019, Long Beach, USA">


    <title> Real-world Sequential Decision Making Workshop </title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/agency.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

  
</head>

<body id="page-top" class="index">
<style>
    /*********************************
     The list of publication items
     *********************************/
/* The list of items */
.abslist { }

/* The item */
.abslist li { }

/* You can define custom styles for plstyle field here. */


/*************************************
 The box that contain BibTeX code
 *************************************/
div.noshow { display: none; }
div.abstract{
	margin-left:5%;
	margin-right:5%;
	margin-top:1.2em;
	margin-bottom:1em;
	border:1px solid silver;
	padding: 0em 1em;
	background: #ffffee;

}
</style>
<script type="text/javascript">
    function toggleAbstract(articleid) {
        var abs = document.getElementById('abs_'+articleid);
        if (abs) {
            if(abs.className.indexOf('abstract') != -1) {
                abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract';
            }
        } else {
            return;
        }
    }
</script>

<!--
div.bibtex {
  margin-right: 0%;
  margin-top: 1.2em;
  margin-bottom: 1em;
  border: 1px solid silver;
  padding: 0em 1em;
  background: #ffffee;
}
div.bibtex pre { font-size: 75%; text-align: left; overflow: auto;  width: 100%; padding: 0em 0em;}</style>
<script type="text/javascript">
    <--
    // Toggle Display of BibTeX
    function toggleBibtex(articleid) {
        var bib = document.getElementById('bib_'+articleid);
        if (bib) {
            if(bib.className.indexOf('bibtex') != -1) {
                bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex';
            }
        } else {
            return;
        }
    }

    </scrip-->

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>

            </div> 

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-middle">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#introduction">Introduction</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#program">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#keynote">Keynote Speakers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#papers">Accepted Papers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#call">CFP</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#dates">Important Dates</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#organization">Organizers</a>
                    </li>
        
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>

    <!-- Header -->
    <header >
        <style>
        header {background-image:url(img/la_skyline.jpg); color:#fff}    
        </style>
        <div class="container">
            <div class="intro-text">
            <div class="intro-heading"> Real-world Sequential Decision Making: Reinforcement Learning and Beyond </br></div>
                            <div class="intro-lead-in"> Real-world Sequential Decision Making Workshop @ <a href="https://icml.cc/Conferences/2019" target=_blank>ICML 2019</a></br>June 14, 2019. Long Beach, USA</div>
                <!--a href="#call" target=_blank class="page-scroll btn btn-xl">Submit a Paper</a-->
            </div>
        </div>
    </header>

    <!-- Introduction Section -->
    <section id="introduction">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Introduction</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                    <p class="large text-muted">
                    From conversational agents to online recommendation to search and advertising, we are already interacting with increasingly sophisticated sequential decision making systems in daily life. Traditionally, sequential decision making research has focused on balancing the exploration-exploitation trade-off, or casting the interaction paradigm under reinforcement / imitation learning dichotomy. We aim to take a holistic view and call for a collective effort to translate principled research ideas into practically relevant solutions in both existing and new domains, such as healthcare, education, safe autonomous vehicles and robots, etc.</br> 
                     </p>

              <p class ="large text-muted"> This workshop aims to bring together researchers from industry and academia in order to describe recent advances and discuss future research directions pertaining to real-world sequential decision making, broadly construed.  We aim to highlight new and emerging research opportunities for the machine learning community that arise from the evolving needs for making decision making theoretically and practically relevant for realistic applications. </br>
                         
                    </p>
                    <p class ="large text-muted"> 
                    Research interest in reinforcement and imitation learning has surged significantly over the past several years, with the empirical successes of self-playing in games and availability of increasingly realistic simulation environments. We believe the time is ripe for the research community to push beyond simulated domains and start exploring research directions that directly address the real-world need for optimal decision making. We are particularly interested in understanding the current theoretical and practical challenges that prevent broader adoption of current policy learning and evaluation algorithms in high-impact applications, across a broad range of domains.  </br>
                  </p>

                </div>
            </div>
        </div>
    </section>

    <!-- Program Section -->
    <section id="program" class="bg-mid-gray">
        <div class="container">

            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Schedule</h2>
                    <h3 class="section-subheading text-muted">
                      Seaside Ballroom, Long Beach Convention Center, Long Beach <br>
                      14:00 pm - 18:00 pm <br>
                      June 14, 2019<br>
                    </h3>
                </div>
            </div>
      
            <div id="schedule">
                <div class = "container w">
                    <h3>Session (tentative)</h3> <br>
                    <div class = "row centered">
                    <table class = "table table-striped" border="2" cellpadding="2" cellspacing="2">                         <tbody>
                    <tr>
                     <td>14:00 - 14:05 </td>
                     <td>Opening Remarks </td>
              
                 </tr>
                 <tr>
                     <td>14:05 - 14:30 </td>
                     <td>Invited Talk </td>
              
                 </tr>
                <tr>
                     <td>14:30 - 15:00 </td>
                     <td>Invited Talk </td>
             
                 </tr>
                  <tr>
                     <td>15:00 - 16:00 </td>
                     <td>Poster Session Part 1 and Coffee Break</td>
         
                 </tr>
                 <tr>
                     <td>16:00 - 16:30 </td>
                     <td>Invited Talk </td>
                     <!-- <td> </td> -->
                 </tr>
                 <tr>
                     <td>16:30 - 17:00 </td>
                     <td>Invited Talk </td>
                     <!-- <td> </td> -->
                 </tr>

                 <tr>
                     <td>17:00 - 17:30 </td>
                     <td>Panel Discussion </td>
                     <!-- <td> </td> -->
                 </tr>
                 <!--<tr>
                     <td>12:00 - 12:30 </td>
                     <td>Invited Talk </td>
                     <-- <td> </td>
                 </tr> -->

                 <tr>
                     <td>17:30 - 18:00 </td>
                     <td>Poster Session - Part 2</td>
                     <!-- <td> </td> -->
                 </tr>
                 
             </tbody>
                
                     </table>
                
                 </div>
                <!--
                 <h3>Afternoon Session</h3> <br>
                 <div class = "row centered">
                     <table class = "table table-striped" border="2" cellpadding="2" cellspacing="2">
                     <tbody>
                         <tr>
                             <td>14:30 - 14:45 </td>
                             <td>Contributed Talk </td>
                         </tr>
                         <tr>
                             <td>14:45 - 15:30 </td>
                             <td>Invited talk</td>
                         </tr>
                         <tr>
                             <td>15:30 - 16:00</td>
                             <td>Afternoon Coffee Break </td>
                         </tr>
                         <tr>
                             <td>16:30 - 16:45 </td>
                             <td>Invited Talk</td>
                         </tr>
                         <tr>
                             <td>16:45 - 17:00 </td>
                             <td>Contributed Talk</td>
                         </tr>
                         <tr>
                             <td>17:00 - 17:15 </td>
                             <td>Contributed Talk</td>
                         </tr>
                          <tr>
                             <td>17:15 - 18:00 </td>
                             <td>Panel Discussion</td>
                         </tr>
                         <tr>
                             <td>18:00 - 18:05 </td>
                             <td>Awards and Closing Remarks</td>
                         </tr>
                         </tbody>
                     </table>
                 
                 </div>
                 -->

              <div class="col-lg-1 text-left">
                &nbsp;
              </div>
          </div>
        </div>
    </section>

    <!-- Keynote Section -->

    <section id="keynote">
         <div class="container">

             <div class="row text-justify">
                 <div class="col-lg-12 text-center">
                     <h2 class="section-heading">Keynote Speakers</h2>  
                </div>
            </div>  

             <div class="row">
                 <div class="col-sm-2">
                     <div class="team-member">
                         <img src="img/emma.jpg" height="150" width="150" class="img-responsive img-circle">
                         <a href="https://cs.stanford.edu/people/ebrun/"><h4>Emma Brunskill</h4></a>
                         <p class="text-muted">Assistant Professor<br/>Stanford University</p>
                     </div>
                 </div>

                  <div class="col-sm-2">
                     <div class="team-member">
                         <img src="img/miro.jpg" height="150" width="150" class="img-responsive img-circle">
                         <a href="https://www.microsoft.com/en-us/research/people/mdudik/"><h4>Miro Dud&iacute;k</h4></a>
                         <p class="text-muted">Principal Researcher<br/>Microsoft Research</p>
                     </div>
                 </div>

                 <!--div class="col-sm-2">
                     <div class="team-member">
                         <img src="img/andreas.jpg" height="150" width="150" class="img-responsive img-circle">
                         <a href="https://las.inf.ethz.ch/krausea"><h4>Andreas Krause</h4></a>
                         <p class="text-muted">Professor<br/>ETH Z&uuml;rich</p>
                     </div>
                 </div-->
                
                 <div class="col-sm-2">
                     <div class="team-member">
                         <img src="img/suchi.jpg" height="150" width="150" class="img-responsive img-circle">
                         <a href="https://suchisaria.jhu.edu/"><h4>Suchi Saria</h4></a>
                         <p class="text-muted">Assistant Professor<br/>Johns Hopkins</p>
                     </div>
                 </div>

                 <div class="col-sm-2">
                     <div class="team-member">
                         <img src="img/dawn.jpg" height="150" width="150" class="img-responsive img-circle">
                         <a href="https://people.orie.cornell.edu/woodard/"><h4>Dawn Woodard</h4></a>
                         <p class="text-muted">Director of Data Science<br/>Uber</p>
                     </div>
                 </div>

                 
             </div>

        </div>
    </section>


    <!-- Papers -->
    <section id="papers"  class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Accepted Papers</h2>
                </div>
            </div>

            <div class="row text-justify">
                <div class="col-md-12">
                    <p class="large text-muted">
                    We are excited to include the following papers in the poster sessions of the workshop:
                      <ul class = "abslist text-left">
                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Online Control with Adversarial Disturbances </strong>
                          	<br><em>Naman Agarwal (Google); Brian Bullins (Princeton University); Elad Hazan (Princeton University and Google Brain); Sham Kakade (University of Washington); Karan Singh (Princeton University)</em><br> 
                          	<a href="javascript:toggleAbstract('agarwal_online')">[abstract]</a>
								<div id="abs_agarwal_online" class="abstract noshow">
								<p class="text-left">
								We study the control of a linear dynamical system with adversarial disturbances (as opposed to statistical noise). The objective we consider is one of regret: we desire an online control procedure that can do nearly as well as that of a procedure that has full knowledge of the disturbances in hindsight. Our main result is an efficient algorithm that provides nearly tight regret bounds for this problem. From a technical standpoint, this work generalizes upon previous work in two main aspects:  our model allows for adversarial noise in the dynamics, and allows for general convex costs.   
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Active Learning for Improving Decision-Making from Imbalanced Data </strong>
                          	<br><em>Iiris Sundin (Aalto University); Peter Schulam (Johns Hopkins University); Eero Siivola (Aalto University); Aki Vehtari (Aalto University); Suchi Saria (Johns Hopkins University); Samuel Kaski (Aalto University)</em><br> 
                          	<a href="javascript:toggleAbstract('sundin_active')">[abstract]</a>
								<div id="abs_sundin_active" class="abstract noshow">
								<p class="text-left">
								This extended abstract considers the reliability of prediction-based decision-making in a task of deciding which action $a$ to take for a target unit after observing its covariates $\tilde{x}$ and predicted outcomes $\hat{p}(\tilde{y} \mid \tilde{x}, a)$. An example case is personalized medicine and the decision of which treatment to give to a patient. A common problem when learning these models from observational data is imbalance, that is, difference in treated/control covariate distributions. We propose to assess the decision-making reliability by estimating the model's Type S error rate, which is the probability of the model inferring the sign of the treatment effect wrong. Furthermore, we use the estimated reliability as a criterion for active learning, in order to collect new (possibly expensive) observations, instead of making a forced choice based on unreliable predictions. We demonstrate the effectiveness of this decision-making aware active learning in two decision-making tasks: in simulated data with binary outcomes and in a medical dataset with synthetic and continuous treatment outcomes.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Challenges of Real-World Reinforcement Learning </strong>
                          	<br><em>Gabriel Dulac-Arnold (Google Research); Daniel Mankowitz (DeepMind); Todd Hester (DeepMind)</em><br> 
                          	<a href="javascript:toggleAbstract('dulac_challenges')">[abstract]</a>
								<div id="abs_dulac_challenges" class="abstract noshow">
								<p class="text-left">
								Reinforcement learning (RL) has proven its worth in a series of artificial domains, and is beginning to show some successes in real-world scenarios. However, much of the research advances in RL are often hard to leverage in real-world systems due to a series of assumptions that are rarely satisfied in practice. We present a set of nine unique challenges that must be addressed to productionize RL to real world problems. For each of these challenges, we specify the exact meaning of the challenge, present some approaches from the literature, and specify some metrics for evaluating that challenge. An approach that addresses all nine challenges would be applicable to a large number of real world problems. We also present an example domain that has been modified to present these challenges as a testbed for practical RL research.
								</p>
								</div>
                          </li>


                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>An Online Algorithm for Smoothed Regression and LQR Control </strong>
                          	<br><em>Gautam Goel (California Institute of Technology); Adam Wierman (California Institute of Technology)</em><br> 
                          	<a href="javascript:toggleAbstract('goel_online')">[abstract]</a>
								<div id="abs_goel_online" class="abstract noshow">
								<p class="text-left">
								We consider Online Convex Optimization (OCO) in the setting where the costs are $m$-strongly convex and the online learner pays a switching cost for changing decisions between rounds. We show that the recently proposed Online Balanced Descent (OBD) algorithm  is constant competitive in this setting, with competitive ratio $3 + O(1/m)$, irrespective of the ambient dimension. We demonstrate the generality of our approach by showing that the OBD framework can be used to construct competitive algorithms for a variety of online problems across learning and control, including online variants of ridge regression, logistic regression, maximum likelihood estimation, and LQR control.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Reinforcement Learning for Sepsis Treatment: Baselines and Analysis </strong>
                          	<br><em>Aniruddh Raghu (University of Cambridge)</em><br> 
                          	<a href="javascript:toggleAbstract('raghu_reinforcement')">[abstract]</a>
								<div id="abs_raghu_reinforcement" class="abstract noshow">
								<p class="text-left">
								In this work, we consider the task of learning effective medical treatment policies for sepsis, a dangerous health condition, from observational data. We examine the performance of various reinforcement learning methodologies on this problem, varying the state representation used. We develop careful baselines for the performance of these methods when using different state representations, standardising the reward function formulation and evaluation methdology. Our results illustrate that simple, tabular Q-learning and Deep Q-Learning both lead to the most effective medical treatment strategies, and that temporal encoding in the state representation aids in discovering improved policies.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Modeling and Interpreting Real-world Human Risk Decision Making with Inverse Reinforcement Learning </strong>
                          	<br><em>Quanying Liu (California Institute of Technology); Haiyan Wu (Chinese Academy of Science ); Anqi Liu (California Institute of Technology)</em><br> 
                          	<a href="javascript:toggleAbstract('raghu_reinforcement')">[abstract]</a>
								<div id="abs_raghu_reinforcement" class="abstract noshow">
								<p class="text-left">
								We model human decision-making behavior in a risk-taking task using inverse reinforcement learning (IRL) for the purposes of understanding the reward function of real human risk decision making. We are the first work that uses IRL to remove reward function behind human risk-taking decision making and interpret human decisions in risk-prone and risk-averse people. We hypothesize that the state history (e.g. rewards and decisions in previous trials) are related to the human reward function, which leads to risk-averse and risk-prone decisions.  In the model, these factors are input to IRL as features, and their reward weights are estimated. The behavioral results confirm the irrational and sub-optimal risk-related decisions. The model results quantify the weight of features and show that risk-prone person tends to decide based on the current bump number, while the risk-averse person relies on burst information from previous trial and the average end status. Our results demonstrate that IRL is an effective tool to model human decision-making behavior and helps interpret human psychological process in risk decisionmaking, and with potential as an assessment tool for risk-taking as well.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling </strong>
                          	<br><em>Andrey Kolobov (Microsoft Research); Yuval Peres (N/A); Cheng Lu (Microsoft Research); Eric Horvitz (Microsoft Research)</em><br> 
                          	<a href="javascript:toggleAbstract('kolobov_staying')">[abstract]</a>
								<div id="abs_kolobov_staying" class="abstract noshow">
								<p class="text-left">
								From traditional Web search to virtual assistants and Web accelerators, services that rely on online information need to keep track of continuing content changes. Many of these services need to explicitly request content updates from remote sources (e.g., web pages), which gives rise to a formidable challenge. A search engine, for example, may track billions of sources, with content change timing or even frequency being initially unknown for most. How should it schedule requests for updates under constraints of network bandwidth? We introduce a novel optimization objective for this setting that has several desirable properties, and efficient algorithms for it with optimality guarantees even in the face of mixed content change observability and initially unknown model parameters. An evaluation on a set of 20M web pages crawled daily for 14 weeks shows the scalability and other properties of our approach. 
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards </strong>
                          	<br><em>Ashvin V Nair (UC Berkeley)</em><br> 
                          	<a href="javascript:toggleAbstract('nair_deep')">[abstract]</a>
								<div id="abs_nair_deep" class="abstract noshow">
								<p class="text-left">
								Connector insertion and many other tasks commonly found in modern manufacturing settings involve complex contact dynamics and friction. Since it is difficult to capture related physical effects with first-order modeling, traditional control methodologies often result in brittle and inaccurate controllers, which have to be manually tuned. Reinforcement learning (RL) methods have been demonstrated to be capable of learning controllers in such environments from autonomous interaction with the environment, but running RL algorithms in the real world poses sample efficiency and safety challenges. Moreover, in practical real-world settings we cannot assume access to perfect state information or dense reward signals. In this paper, we consider a variety of difficult industrial insertion tasks with visual inputs and different natural reward specifications, namely sparse rewards and goal images. We show that methods that combine RL with prior information, such as classical controllers or demonstrations, can solve these tasks directly by real-world interaction.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Practical Complications in Applying Bayesian Optimization to Understand Tradeoffs between Substrate Fabrication Processes </strong>
                          	<br><em>Sajad Haghanifar (University of Pittsburgh); Bolong Cheng (SigOpt); Mike Mccourt (SigOpt); Paul Leu (University of Pittsburgh)</em><br> 
                          	<a href="javascript:toggleAbstract('haghanifar_practical')">[abstract]</a>
								<div id="abs_haghanifar_practical" class="abstract noshow">
								<p class="text-left">
								Designing an effective materials fabrication process demands searching the realm of possible fabrication processes to understand how desired properties are impacted by the specifications of the process. We apply active learning in the context of Bayesian optimization to propose an adaptive experimental design -- our goal is to identify processes with the most desirable properties in a sample efficient fashion. In reflecting on our cross-disciplinary industry-academic collaboration between materials scientists and mathematicians, we discuss our approach to managing practical concerns and complications; these issues may be rarely addressed in theoretical literature but were necessary for our circumstances.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>On-Policy Imitation Learning from an Improving Supervisor </strong>
                          	<br><em>Ashwin Balakrishna (UC Berkeley); Brijen Thananjeyan (UC Berkeley)</em><br> 
                          	<a href="javascript:toggleAbstract('balakrishna')">[abstract]</a>
								<div id="abs_balakrishna" class="abstract noshow">
								<p class="text-left">
								Most on-policy imitation algorithms, such as DAgger, are designed for learning with a fixed supervisor. However, there are many settings in which the supervisor improves during policy learning, such as when the supervisor is a human performing a novel task or an improving algorithmic controller. We consider learning from an “improving supervisor” and derive a bound on the static-regret of online gradient descent when a converging supervisor policy is used. We present an on-policy imitation learning algorithm, Follow the Improving Teacher (FIT), which uses a deep model-based reinforcement learning (deepMBRL) algorithm to provide the sample complexity benefits of model-based methods but enable faster training and evaluation via distillation into a reactive controller.  We evaluate FIT with experiments on the Reacher and Pusher MuJoCodomains using the deep MBRL algorithm, PETS, as the improving supervisor. To the best of our knowledge, this work is the first to formally consider the setting of an improving supervisor in on-policy imitation learning.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Importance Sampling Policy Evaluation with an Estimated Behavior Policy </strong>
                          	<br><em>Josiah Hanna (UT Austin); Scott Niekum (UT Austin); Peter Stone (UT Austin)</em><br> 
                          	<a href="javascript:toggleAbstract('hanna_importance')">[abstract]</a>
								<div id="abs_hanna_importance" class="abstract noshow">
								<p class="text-left">
								We consider the problem of off-policy evaluation in Markov decision processes. Off-policy evaluation is the task of evaluating the expected return of one policy with data generated by a different, behavior policy. Importance sampling is a technique for off-policy evaluation that re-weights off-policy returns to account for differences in the likelihood of the returns between the two policies. In this paper, we study importance sampling with an estimated behavior policy where the behavior policy estimate comes from the same set of data used to compute the importance sampling estimate. We find that this estimator often lowers the mean squared error of off-policy evaluation compared to importance sampling with the true behavior policy or using a behavior policy that is estimated from a separate data set. Intuitively, estimating the behavior policy in this way corrects for error due to sampling in the action-space. Our empirical results also extend to other popular variants of importance sampling.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Generalized Off-Policy Actor-Critic </strong>
                          	<br><em>Shangtong Zhang (University of Oxford); Wendelin Boehmer (University of Oxford); Shimon Whiteson (University of Oxford)</em><br> 
                          	<a href="javascript:toggleAbstract('zhang_generalized')">[abstract]</a>
								<div id="abs_zhang_generalized" class="abstract noshow">
								<p class="text-left">
								We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in the continuing reinforcement learning (RL) setting. Compared to the commonly used excursion objective, which can be misleading about the performance of the target policy when deployed, our new objective better predicts such performance. We prove the Generalized Off-Policy Policy Gradient Theorem to compute the policy gradient of the counterfactual objective and use an emphatic approach to get an unbiased sample from this policy gradient, yielding the Generalized Off-Policy Actor-Critic (Geoff-PAC) algorithm. We demonstrate the merits of Geoff-PAC over existing algorithms in Mujoco robot simulation tasks, the first empirical success of emphatic algorithms in prevailing deep RL benchmarks.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Deep Residual Reinforcement Learning </strong>
                          	<br><em>Shangtong Zhang (University of Oxford); Wendelin Boehmer (University of Oxford); Shimon Whiteson (University of Oxford)</em><br> 
                          	<a href="javascript:toggleAbstract('zhang_deep')">[abstract]</a>
								<div id="abs_zhang_deep" class="abstract noshow">
								<p class="text-left">
								We revisit residual algorithms in both model-free and model-based reinforcement learning settings. We propose the bidirectional target network technique to stabilize residual algorithms, yielding a residual version of DDPG that significantly outperforms vanilla DDPG in the DeepMind Control Suite benchmark. Moreover, we find the residual algorithm an effective approach to the distribution mismatch problem in model-based planning. Compared with the existing TD(k) method, our residual-based method makes weaker assumptions about the model and yields a greater performance boost.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches </strong>
                          	<br><em>Wen Sun (Carnegie Mellon University)*; Nan Jiang (University of Illinois at Urbana-Champaign); Akshay Krishnamurthy (Microsoft); Alekh Agarwal (Microsoft); John Langford (Microsoft)</em><br> 
                          	<a href="javascript:toggleAbstract('sun_model')">[abstract]</a>
								<div id="abs_sun_model" class="abstract noshow">
								<p class="text-left">
								We study the sample complexity of model-based reinforcement learning (henceforth RL) in gen- eral contextual decision processes that require strategic exploration to find a near-optimal policy. We design new algorithms for RL with a generic model class and analyze their statistical properties. Our algorithms have sample complexity governed by a new structural parameter called the witness rank, which we show to be small in several set- tings of interest, including factored MDPs. We also show that the witness rank of a problem is never larger than the recently proposed Bellman rank parameter governing the sample complexity of the model-free algorithm OLIVE (Jiang et al., 2017), the only other provably sample-efficient al- gorithm for global exploration at this level of gen- erality. Focusing on the special case of factored MDPs, we prove an exponential lower bound for a general class of model-free approaches, including OLIVE, which when combined with our algorith- mic results demonstrates exponential separation between model-based and model-free RL in some rich-observation settings.
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Regret Bounds for Thompson Sampling in Restless Bandit Problems </strong>
                          	<br><em>Young Hun Jung (University of Michigan); Ambuj Tewari (University of Michigan)</em><br> 
                          	<a href="javascript:toggleAbstract('jung_regret')">[abstract]</a>
								<div id="abs_jung_regret" class="abstract noshow">
								<p class="text-left">
								Restless bandit problems are instances of non-stationary multi-armed bandits. There are plenty of results from the optimization perspective, which aims to efficiently find a near-optimal policy when system parameters are known, but the learning perspective where the parameters are unknown is rarely investigated. In this paper, we analyze the performance of Thompson sampling in restless bandits with unknown parameters. We consider a general policy map to define our competitor and prove an $\tilde{O}(\sqrt{T})$ Bayesian regret bound. Our competitor is flexible enough to represent various benchmarks including the best fixed action policy, the optimal policy, the Whittle index policy, or the myopic policy. The empirical results also support our theoretical findings. 
								</p>
								</div>
                          </li>

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>Dynamic Interruption Policies for Reinforcement Learning Game Playing Using Multi-Sampling Multi-Armed Bandits </strong>
                          	<br><em>Fa Wu (Zhejiang Demetics Medical Technology Co.,Ltd.); Rendong Chen (Zhejiang University); Shouchao Wang (Zhejiang Demetics Medical Technology Co.,Ltd.); Ningzi Zhang (Zhejiang Demetics Medical Technology Co.,Ltd.); Sunyun Qi (Zhejiang Demetics Medical Technology Co.,Ltd.); Dexing Kong (Zhejiang University)</em><br> 
                          	<a href="javascript:toggleAbstract('wu_dynamic')">[abstract]</a>
								<div id="abs_wu_dynamic" class="abstract noshow">
								<p class="text-left">
								In many reinforcement learning (RL) game tasks, episodes should be interrupted after a certain time, as the agent could sometimes fall into a deadlock state. This paper provides a dynamic interruption policy in RL game training via a newly defined multi-sampling multi-armed bandit (MSMAB) model, and offers an efficient algorithm named Exp3.P.MS for the new bandit setting. The experimental results show that the dynamic interruptions can adapt to the performance of the RL agent and spur a fast learning in game playing. 
								</p>
								</div>
                          </li>                      

                          <li style = "margin-top:1px;margin-bottom:1px"> <strong>A Communication Efficient Hierarchical Distributed Optimization Algorithm for Multi-Agent Policy Evaluation </strong>
                          	<br><em>Jineng Ren (University of Minnesota Twin Cities); Jarvis Haupt (UMN)</em><br> 
                          	<a href="javascript:toggleAbstract('ren_communication')">[abstract]</a>
								<div id="abs_ren_communication" class="abstract noshow">
								<p class="text-left">
								Policy evaluation problems in multi-agent reinforcement learning (MARL) have attracted growing interest recently. In these settings, agents collaborate to learn the value of a given policy with private local rewards and jointly observed state-action pairs. However, most fully decentralized algorithms treat each agent equally, without considering the communication structure of the agents over a given network, and the corresponding effects on communication efficiency. In this paper, we propose a hierarchical distributed algorithm that differentiates the roles of each of the agents during the evaluation process. This allows us to freely choose various mixing schemes (and corresponding mixing matrices that are not necessarily doubly stochastic), in order to reduce the communication cost, while still maintaining convergence at rates as fast as or even faster than the previous approaches. Theoretically, we show the proposed method, which contains existing distributed methods as a special case, achieves the same order of convergence rate as state-of-the-art methods. Numerical experiments on real datasets demonstrate the superior performances of our approach over other advanced algorithms in terms of convergence and total communication efficiency.  
								</p>
								</div>
                          </li>                      
                      </ul>                   
                    </p> 
               </div>
            </div>
        
            <div class="row centered">
                <ul>
            </ul>
            </div>
            
      

        </div>
    </section>




    <!-- call for papers -->
    <section id="call" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Call for Papers</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row text-justify">
                <div class="col-md-12">
                    <p class="large text-muted">
                    Papers submitted to the workshop should be up to five pages long excluding references and appendix, and in ICML 2019 format.  As the review process is not blind, authors can reveal their identity in their submissions. All inquiries may be sent to 
                    <a href="mailto:RealWorldSDM@gmail.com?Subject=RealWorldSDMWorkshop" target="_top">RealWorldSDM@gmail.com</a>
.                   </p>

                    <p class="large text-muted">
                    <!--   Submissions website: TBA. -->
                      Submissions page: <a href="https://cmt3.research.microsoft.com/RWSDM2019">Real-world Sequential Decision Making Workshop 2019</a>.
                    </p> 
                   
                    
                  <p class="large text-muted">
                      <!--<b>Note on <i>possible topics</i> of submissions:</b> -->We invite researchers to submit both theoretical and applied work along several possible dimensions:
                      </p>

                      <ul>
                    
                      <li style = "margin-top:2px;margin-bottom:2px"> <p class="large text-muted"> Application papers: applications of learning-based techniques to real-life sequential decision making (robotics, healthcare, education, transportation & energy, smart-grids, sustainability, NLP, social media & advertising, agriculture, manufacturing, economics and policy) </p> </li>
                      
                    
                    
                     <li style = "margin-top:2px;margin-bottom:2px"> <p class="large text-muted"> Method papers that address real-world desiderata and concerns: safety, reliable decision making, theoretical guarantees, verifiability & interpretability, data-efficiency, data-heterogeneity, efficient exploration, counterfactual reasoning and off-policy evaluation, cost function design, efficient implementations to large-scale systems</p></li>
                     
                    
                    
                    <li style = "margin-top:2px;margin-bottom:2px"> <p class="large text-muted">Cross-boundary papers along the theme of RL+X  where X indicate areas not commonly viewed as RL in contemporary research. We would like to encourage researchers to explore the interface between traditional RL with:</p> 
                        <ul>
                          <li style = "margin-top:1px;margin-bottom:1px"> <p class="large text-muted">Other related areas in machine learning including but not limited to: imitation learning, transfer learning, active learning, structured prediction, off-policy learning, fairness in ML, privacy</p> </li>
                          <li style = "margin-top:1px;margin-bottom:1px"><p class="large text-muted">Areas outside of machine learning including but not limited to: control theory & dynamical systems, formal methods, causal inference, game-theory, operations research, systems research, human-computer interactions, human behavior modeling</p> </li>
                        </ul>
                      </li>
                    
 
                    </ul>
                  
  
               </div>
            </div>

        </div>
</section>

    <!-- Dates Section -->
    <section id="dates">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Key Dates</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>
            <div class="row">
              <div class="col-lg-4 text-left">
                &nbsp;
              </div>
            <div class="col-lg-6 text-left">
                <div class="col-md-12">
                      <p class="large text-muted">
                          <b>Paper Submission Deadline:</b> May 20, 2019, 11:59 PM PST
                      </p>
                      <p class="large text-muted">
                          <b>Author Notification:</b> May 31, 2019, 11:59 PM PST
                      </p>
                      <p class="large text-muted">
                        <b>Final Version:</b> June 9, 2019, 11:59 PM PST 
                      </p>
                      <p class="large text-muted">
                      <b>Workshop:</b> June 14 or 15, 2019
                      </p>
                </div>
            </div>
          </div>
        </div>
    </section>

  <!-- Organization Section -->
    <section id="organization" class="bg-mid-gray">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Workshop Organizers</h2>
                    <!--h3 class="section-subheading text-muted">Lorem ipsum dolor sit amet consectetur.</h3-->
                </div>
            </div>

            <div class="row">
                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/hoang.jpg" class="img-responsive img-circle" alt="">
                        <a href="http://hoangle.info"> <h4>Hoang M. Le</h4> </a>
                        <p class="text-muted">Caltech</p>
                    </div>
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/yisong.jpg" class="img-responsive img-circle" alt="">
                        <a href="http://www.yisongyue.com/"><h4>Yisong Yue</h4></a>
                        <p class="text-muted"> Caltech</p>
                    </div>
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/adith.jpg" class="img-responsive img-circle" alt="">
                        <a href="https://www.microsoft.com/en-us/research/people/adswamin/"><h4>Adith Swaminathan</h4></a>
                        <p class="text-muted"> Microsoft Research AI</p>
                    </div>
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/byron.jpg" class="img-responsive img-circle" alt="">
                        <a href="https://www.cc.gatech.edu/~bboots3/"><h4>Byron Boots</h4></a>
                        <p class="text-muted"> Georgia Tech & NVIDIA</p>
                    </div>
                </div>

                <div class="col-sm-2">
                    <div class="team-member">
                        <img src="img/ching-an.jpg" class="img-responsive img-circle" alt="">
                        <a href="https://robotlearning.gatech.edu/people/ching-ancheng/"><h4>Ching-An Cheng</h4></a>
                        <p class="text-muted">Georgia Tech</p>
                    </div>
                </div>
                <!--
                <div class="col-sm-2">
                   &nbsp;
                </div>
                -->
            </div>
            
        </div>
    </section>

    <!-- Contact Section -->
<!--     <section id="contact">
        <div class="container">
            <div class="row">
                <div class="col-lg-12 text-center">
                    <h2 class="section-heading">Sponsor</h2>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-8">
                    <img src="img/ubereng_new_mailchimp_header.jpg" class="img-rounded" alt=""> 

                </div>
                <div class="col-lg-4">
                     <br> <br /> 
                    <a href="https://www.uber.com/"><h4>Uber Engineering</h4>  </a>
                    <h5 class="text-muted">Transportation as reliable as running water, everywhere for everyone.</h5>
                </div>
                    <p class="help-block text-danger"></p>           
                </div>
        </div>
    </section> -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
    <script src="js/classie.js"></script>
    <script src="js/cbpAnimatedHeader.js"></script>

    <!-- Contact Form JavaScript -->
    <script src="js/contact_me.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/agency.js"></script>

</body>

</html>
